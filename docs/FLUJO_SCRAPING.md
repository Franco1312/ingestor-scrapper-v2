# ğŸ•·ï¸ CÃ³mo Scrapear tu Primera PÃ¡gina - GuÃ­a Paso a Paso

## ğŸ¯ Â¿QuÃ© hace el `example_spider`?

El spider `example_spider.py` scrapea `https://example.com` y extrae el tÃ­tulo de la pÃ¡gina.

## ğŸ“‹ Flujo Completo (Paso a Paso)

### **FASE 1: Scrapy Inicia el Spider**

1. **Ejecutas el comando:**
   ```bash
   scrapy crawl example
   ```

2. **Scrapy lee el spider:**
   - Ve que `name = "example"`
   - Lee `start_urls = ["https://example.com"]`
   - Hace una peticiÃ³n HTTP a esa URL
   - Cuando recibe la respuesta, llama automÃ¡ticamente a `parse(response)`

---

### **FASE 2: El mÃ©todo `parse()` se ejecuta**

El mÃ©todo `parse()` es donde ocurre toda la magia. Veamos quÃ© hace:

```python
def parse(self, response):
    # response es un objeto de Scrapy que contiene:
    # - response.url = "https://example.com"
    # - response.text = "<html>...</html>" (HTML completo)
    # - response.status = 200 (cÃ³digo HTTP)
```

**Paso 2.1: Crear los adapters (lÃ­neas 62-65)**
```python
# Crea 3 "adaptadores" que convierten Scrapy â†’ nuestro dominio
fetcher = AdapterScrapyFetcher(response)  # Convierte Response â†’ Page
parser = AdapterBs4Parser()              # Convierte HTML â†’ Items
output = AdapterStdoutOutput()           # Imprime los items en consola
```

**Paso 2.2: Crear el caso de uso (lÃ­neas 67-70)**
```python
# Crea el "orquestador" que coordina todo el flujo
use_case = CrawlAndParseUseCase(
    fetcher=fetcher,  # Le pasamos el fetcher
    parser=parser,    # Le pasamos el parser
    output=output     # Le pasamos el output
)
```

**Paso 2.3: Ejecutar el caso de uso (lÃ­nea 76)**
```python
items = use_case.execute(response.url)  # Â¡AquÃ­ comienza el scraping!
```

---

### **FASE 3: El Use Case Orquesta el Flujo**

Cuando ejecutas `use_case.execute(url)`, esto es lo que pasa **dentro**:

**Paso 3.1: Fetch (Obtener HTML) - LÃ­nea 68**
```python
page: Page = self.fetcher.fetch(url)
```
- Llama a `AdapterScrapyFetcher.fetch()`
- Este adapter toma `response.text` (el HTML) y crea un objeto `Page`
- **Resultado:** `Page(url="https://example.com", html="<html>...</html>", status_code=200)`

**Paso 3.2: Parse (Extraer Datos) - LÃ­nea 74**
```python
items: List[Item] = self.parser.parse(page.html, page.url)
```
- Llama a `AdapterBs4Parser.parse()` con el HTML
- Este adapter busca `<title>` y `<h1>` en el HTML
- **Resultado:** `[Item(title="Example Domain", content="...", url="https://example.com")]`

**Paso 3.3: Output (Mostrar Resultados) - LÃ­nea 79**
```python
self.output.emit(items)
```
- Llama a `AdapterStdoutOutput.emit()` con los items
- Imprime en consola: `Found 1 item(s): [1] Title: Example Domain`

---

### **FASE 4: Devolver Resultados**

El `use_case.execute()` devuelve la lista de `items`, y el spider los loguea:

```python
logger.info("Successfully parsed %s", response.url)
logger.info("Extracted title: %s", title)
```

---

## ğŸ” Flujo Visual Simplificado

```
1. Scrapy llama a parse(response)
   â”‚
   â”œâ”€ 2. Crear adapters
   â”‚  â”œâ”€ fetcher = AdapterScrapyFetcher(response)
   â”‚  â”œâ”€ parser = AdapterBs4Parser()
   â”‚  â””â”€ output = AdapterStdoutOutput()
   â”‚
   â”œâ”€ 3. Crear use case
   â”‚  â””â”€ use_case = CrawlAndParseUseCase(fetcher, parser, output)
   â”‚
   â”œâ”€ 4. Ejecutar use case
   â”‚  â”‚
   â”‚  â”œâ”€ 4.1 FETCH: fetcher.fetch() â†’ Page
   â”‚  â”‚         (Response â†’ Page entity)
   â”‚  â”‚
   â”‚  â”œâ”€ 4.2 PARSE: parser.parse() â†’ List[Item]
   â”‚  â”‚         (HTML â†’ Items estructurados)
   â”‚  â”‚
   â”‚  â””â”€ 4.3 OUTPUT: output.emit() â†’ Imprime
   â”‚            (Items â†’ Consola)
   â”‚
   â””â”€ 5. Recibir items y loguear
      â””â”€ items = [Item(title="Example Domain", ...)]
```

---

## ğŸš€ CÃ³mo Ejecutarlo

```bash
# 1. Activar entorno virtual
source .venv/bin/activate

# 2. Ejecutar el spider
scrapy crawl example
```

**Salida esperada:**
```
INFO: Found 1 item(s):
INFO:   [1] Title: Example Domain
INFO:      Content: Example Domain
INFO:      URL: https://example.com
INFO: Successfully parsed https://example.com
INFO: Extracted title: Example Domain
```

---

## ğŸ”§ CÃ³mo Modificarlo para Otra PÃ¡gina

### OpciÃ³n 1: Modificar el spider existente

```python
class ExampleSpider(scrapy.Spider):
    name = "example"
    allowed_domains = ["tusitio.com"]  # Cambia el dominio
    start_urls = ["https://tusitio.com"]  # Cambia la URL
```

### OpciÃ³n 2: Crear un nuevo spider

```bash
scrapy genspider mi_spider tusitio.com
```

Luego edita `mi_spider.py` para seguir el mismo patrÃ³n que `example_spider`.

---

## ğŸ’¡ Conceptos Clave

1. **Spider (`example_spider.py`):** Punto de entrada. Scrapy lo ejecuta.
2. **Parse method:** Se ejecuta automÃ¡ticamente cuando Scrapy recibe una respuesta.
3. **Adapters:** Convierten frameworks (Scrapy) â†’ nuestro dominio (Page, Item).
4. **Use Case:** Orquesta el flujo: fetch â†’ parse â†’ output.
5. **Entities:** Modelos de datos simples (Page, Item).

---

## â“ Preguntas Frecuentes

**P: Â¿Por quÃ© no scrapea directamente en el `parse()`?**
R: Por separaciÃ³n de responsabilidades. El `parse()` solo coordina; la lÃ³gica estÃ¡ en el use case.

**P: Â¿Puedo scrapear mÃºltiples URLs?**
R: SÃ­, puedes agregar mÃ¡s URLs a `start_urls` o hacer `yield scrapy.Request(url, callback=self.parse)`.

**P: Â¿CÃ³mo extraigo mÃ¡s datos?**
R: Modifica `AdapterBs4Parser.parse()` para buscar mÃ¡s elementos HTML.

