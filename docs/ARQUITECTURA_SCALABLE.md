# üèóÔ∏è Arquitectura Escalable - ingestor-scrapper-v2

## üìã Descripci√≥n General

Este documento explica la arquitectura escalable del proyecto, dise√±ada para soportar m√∫ltiples sitios y formatos (HTML, CSV, Excel, PDF) manteniendo Clean Architecture (Ports & Adapters).

## üéØ Objetivos de la Arquitectura

1. **Mantenibilidad**: Separaci√≥n clara de responsabilidades
2. **Escalabilidad**: F√°cil agregar nuevos sitios y formatos
3. **Flexibilidad**: Soporte para m√∫ltiples formatos (HTML, CSV, Excel, PDF)
4. **Compatibilidad**: Los spiders existentes siguen funcionando

## üìê Diagrama de Flujo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SPIDER                                   ‚îÇ
‚îÇ  (bcra_spider, scrapethissite_spider, universal_spider)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  USE CASE                                    ‚îÇ
‚îÇ  (BcraUseCase, ScrapeThisSiteUseCase, UniversalIngestUseCase)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DOCUMENT FETCHER                                ‚îÇ
‚îÇ  (AdapterScrapyDocumentFetcher)                             ‚îÇ
‚îÇ  - Detecta Content-Type (HTML, CSV, Excel, PDF)             ‚îÇ
‚îÇ  - Crea Document entity                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PARSER ROUTER                                   ‚îÇ
‚îÇ  (ParserRouter)                                              ‚îÇ
‚îÇ  - Selecciona parser seg√∫n ContentType                      ‚îÇ
‚îÇ  - Usa PARSER_REGISTRY                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PARSER                                      ‚îÇ
‚îÇ  - HtmlParser (AdapterBs4Parser)                            ‚îÇ
‚îÇ  - TabularParser (AdapterCsvParser, AdapterExcelParser)      ‚îÇ
‚îÇ  - PdfParser (AdapterPdfParser)                             ‚îÇ
‚îÇ  - Convierte Document ‚Üí List[Record]                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                NORMALIZER                                    ‚îÇ
‚îÇ  (AdapterBcraNormalizer, AdapterGenericNormalizer)          ‚îÇ
‚îÇ  - Convierte List[Record] ‚Üí List[Item]                      ‚îÇ
‚îÇ  - Mapea campos espec√≠ficos del sitio                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 OUTPUT PORT                                  ‚îÇ
‚îÇ  (AdapterStdoutOutput, AdapterJsonOutput)                   ‚îÇ
‚îÇ  - Emite List[Item]                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üß© Componentes Principales

### 1. Core (Dominio)

#### Entities

- **`Item`**: Entidad del dominio para items extra√≠dos (title, content, url)
- **`Page`**: Entidad para p√°ginas HTML (mantiene compatibilidad)
- **`Document`**: Entidad gen√©rica para documentos (soporta m√∫ltiples formatos)
- **`Record`**: Entidad intermedia del parser (data dict, source_url, fetched_at)
- **`ContentType`**: Enum para tipos de contenido (HTML, CSV, XLS, XLSX, PDF, UNKNOWN)

#### Ports

- **`HtmlFetcher`**: Puerto para obtener HTML (compatibilidad)
- **`DocumentFetcher`**: Puerto gen√©rico para obtener documentos
- **`HtmlParser`**: Puerto para parsear HTML ‚Üí Records
- **`TabularParser`**: Puerto para parsear CSV/Excel ‚Üí Records
- **`PdfParser`**: Puerto para parsear PDF ‚Üí Records
- **`Normalizer`**: Puerto para normalizar Records ‚Üí Items
- **`OutputPort`**: Puerto para emitir Items (mantiene compatibilidad)

### 2. Application (Casos de Uso)

- **`CrawlAndParseUseCase`**: Caso de uso gen√©rico (compatibilidad)
- **`BcraUseCase`**: Caso de uso espec√≠fico para BCRA
- **`ScrapeThisSiteUseCase`**: Caso de uso espec√≠fico para ScrapeThisSite
- **`UniversalIngestUseCase`**: Caso de uso gen√©rico con ParserRouter
- **`ParserRouter`**: Componente que selecciona parser seg√∫n ContentType

### 3. Adapters (Implementaciones)

#### Fetchers

- **`AdapterScrapyFetcher`**: Implementa HtmlFetcher (compatibilidad)
- **`AdapterScrapyDocumentFetcher`**: Implementa DocumentFetcher, detecta ContentType
- **`AdapterHttpFetcher`**: Stub para requests (no Scrapy)

#### Parsers

- **`AdapterBs4Parser`**: Parser HTML gen√©rico (stub, requiere beautifulsoup4)
- **`AdapterBcraParser`**: Parser HTML espec√≠fico para BCRA (existente, funciona)
- **`AdapterScrapeThisSiteParser`**: Parser HTML espec√≠fico (existente, funciona)
- **`AdapterCsvParser`**: Parser CSV (stub b√°sico)
- **`AdapterExcelParser`**: Parser Excel (stub, requiere openpyxl/xlrd)
- **`AdapterPdfParser`**: Parser PDF (stub, requiere pdfplumber)

#### Normalizers

- **`AdapterBcraNormalizer`**: Normaliza Records de BCRA ‚Üí Items
- **`AdapterGenericNormalizer`**: Normaliza Records gen√©ricos ‚Üí Items (fallback)

#### Outputs

- **`AdapterStdoutOutput`**: Emite Items a consola
- **`AdapterJsonOutput`**: Emite Items como JSON a archivo

#### Registry

- **`PARSER_REGISTRY`**: Registro centralizado de parsers por ContentType

### 4. Interface (Spiders)

- **`BcraSpider`**: Spider espec√≠fico para BCRA (funciona)
- **`ScrapeThisSiteSpider`**: Spider espec√≠fico para ScrapeThisSite (funciona)
- **`UniversalSpider`**: Spider gen√©rico que usa UniversalIngestUseCase

## üìù C√≥mo Agregar un Nuevo Sitio

### Paso 1: Crear Parser (si es necesario)

Si el sitio requiere parsing espec√≠fico, crea un nuevo parser que implemente `HtmlParser`:

```python
# adapters/parser_mi_sitio.py
from ingestor_scrapper.core.entities import Document, Record
from ingestor_scrapper.core.ports import HtmlParser

class AdapterMiSitioParser(HtmlParser):
    def parse(self, document: Document) -> List[Record]:
        # L√≥gica de parsing espec√≠fica
        records = []
        # ... extraer datos ...
        return records
```

### Paso 2: Crear Normalizer (si es necesario)

Si el sitio requiere normalizaci√≥n espec√≠fica:

```python
# adapters/normalizer_mi_sitio.py
from ingestor_scrapper.core.entities import Item, Record
from ingestor_scrapper.core.ports import Normalizer

class AdapterMiSitioNormalizer(Normalizer):
    def normalize(self, records: List[Record]) -> List[Item]:
        items = []
        # ... mapear Records a Items ...
        return items
```

### Paso 3: Crear Use Case (opcional)

Si necesitas l√≥gica espec√≠fica del sitio:

```python
# application/mi_sitio_use_case.py
from ingestor_scrapper.application.use_cases import UseCase
from ingestor_scrapper.core.ports import DocumentFetcher, Normalizer, OutputPort
from ingestor_scrapper.core.ports import HtmlParser

class MiSitioUseCase(UseCase):
    def __init__(self, fetcher, parser, normalizer, output):
        # ...
```

### Paso 4: Crear Spider

```python
# interface/spiders/mi_sitio_spider.py
class MiSitioSpider(scrapy.Spider):
    name = "mi_sitio"
    start_urls = ["https://mi-sitio.com"]
    
    def parse(self, response):
        fetcher = AdapterScrapyDocumentFetcher(response)
        parser = AdapterMiSitioParser()
        normalizer = AdapterMiSitioNormalizer()
        output = AdapterJsonOutput()
        
        use_case = MiSitioUseCase(fetcher, parser, normalizer, output)
        use_case.execute(response.url)
```

## üìä C√≥mo Agregar un Nuevo Formato

### Paso 1: Implementar el Parser Port

Si el formato no existe, primero crea el Port (si es necesario):

```python
# core/ports.py
class MiFormatoParser(ABC):
    @abstractmethod
    def parse(self, document: Document) -> List[Record]:
        pass
```

### Paso 2: Crear el Adapter

```python
# adapters/parser_mi_formato.py
from ingestor_scrapper.core.entities import Document, Record
from ingestor_scrapper.core.ports import MiFormatoParser

class AdapterMiFormatoParser(MiFormatoParser):
    def parse(self, document: Document) -> List[Record]:
        # TODO: Instalar librer√≠a necesaria
        # pip install mi-libreria
        records = []
        # ... parsear documento ...
        return records
```

### Paso 3: Agregar al Registry

```python
# adapters/parsers/registry.py
from ingestor_scrapper.adapters.parsers.mi_formato import (
    AdapterMiFormatoParser,
)
from ingestor_scrapper.core.entities import ContentType

_MI_FORMATO_PARSER = AdapterMiFormatoParser()

PARSER_REGISTRY = {
    # ... existentes ...
    ContentType.MI_FORMATO: _MI_FORMATO_PARSER,
}
```

### Paso 4: Agregar ContentType (si es necesario)

```python
# core/entities.py
class ContentType(Enum):
    # ... existentes ...
    MI_FORMATO = "mi_formato"
```

## ‚úÖ Checklist para Nuevos Parsers

- [ ] Detectar Content-Type (en DocumentFetcher o Parser)
- [ ] Crear Adapter del Port correspondiente
- [ ] Implementar m√©todo `parse(document: Document) -> List[Record]`
- [ ] Agregar al `PARSER_REGISTRY`
- [ ] (Opcional) Crear Normalizer espec√≠fico
- [ ] Probar con `UniversalSpider`
- [ ] Documentar dependencias requeridas (TODO en c√≥digo)

## üîß Dependencias Futuras

Las siguientes librer√≠as se pueden agregar cuando se implementen los parsers:

### HTML Parsing
```bash
pip install beautifulsoup4 lxml
```
- **beautifulsoup4**: Para parsing HTML gen√©rico
- **lxml**: Parser m√°s r√°pido (opcional)

### Excel Parsing
```bash
pip install openpyxl
# Para XLS legacy:
pip install xlrd  # Nota: usar versi√≥n < 2.0 para XLS
```
- **openpyxl**: Para archivos .xlsx
- **xlrd**: Para archivos .xls (legacy)

### PDF Parsing
```bash
pip install pdfplumber
# Alternativa:
pip install tabula-py  # Requiere Java
```
- **pdfplumber**: Recomendado (puro Python)
- **tabula-py**: Alternativa (requiere Java)

### HTTP Requests (alternativa a Scrapy)
```bash
pip install requests
```
- **requests**: Para fetching sin Scrapy (scripts standalone)

## üéì Ejemplos de Uso

### Ejemplo 1: Spider Existente (BCRA)

```python
# BcraSpider sigue funcionando igual
scrapy crawl bcra
```

### Ejemplo 2: Universal Spider

```python
# Maneja m√∫ltiples formatos autom√°ticamente
scrapy crawl universal -a url="https://example.com/data.html"
scrapy crawl universal -a url="https://example.com/data.csv"
scrapy crawl universal -a url="https://example.com/data.xlsx"
```

### Ejemplo 3: Nuevo Sitio con Parsing Espec√≠fico

```python
# 1. Crear parser espec√≠fico
class AdapterNuevoSitioParser(HtmlParser):
    def parse(self, document: Document) -> List[Record]:
        # Parsing espec√≠fico
        pass

# 2. Crear spider
class NuevoSitioSpider(scrapy.Spider):
    name = "nuevo_sitio"
    # ... usar UniversalIngestUseCase o UseCase espec√≠fico ...
```

## üîç Notas Importantes

1. **Compatibilidad**: Los spiders existentes (`bcra`, `scrapethissite`) siguen funcionando
2. **Stubs**: Muchos parsers son stubs con TODOs - no est√°n completamente implementados
3. **Normalizers**: Use normalizers espec√≠ficos cuando sea posible; `AdapterGenericNormalizer` es fallback
4. **Content-Type Detection**: Se hace autom√°ticamente en `AdapterScrapyDocumentFetcher`
5. **ParserRouter**: Selecciona autom√°ticamente el parser correcto seg√∫n ContentType

## üìö Archivos Clave

- **`core/entities.py`**: Value Objects (Document, Record, ContentType)
- **`core/ports.py`**: Interfaces (DocumentFetcher, HtmlParser, TabularParser, etc.)
- **`application/parser_router.py`**: Router que selecciona parsers
- **`application/universal_ingest_use_case.py`**: Use case gen√©rico
- **`adapters/registry.py`**: Registro centralizado de parsers
- **`interface/spiders/universal_spider.py`**: Spider de ejemplo

## üöÄ Pr√≥ximos Pasos

1. Implementar parsers reales (BeautifulSoup4, openpyxl, pdfplumber)
2. Agregar m√°s normalizers espec√≠ficos por sitio
3. Mejorar detecci√≥n de Content-Type
4. Agregar tests unitarios
5. Documentar casos de uso espec√≠ficos

