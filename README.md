# ingestor-scrapper

Un proyecto de Scrapy con Clean Architecture (Ports & Adapters) para aprender web scraping desde cero, pero con una estructura escalable y ordenada desde el inicio.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un scaffold mÃ­nimo pero funcional de Scrapy siguiendo los principios de Clean Architecture (Hexagonal Architecture). La estructura estÃ¡ diseÃ±ada para crecer sin necesidad de reestructurar todo el proyecto cuando se agreguen nuevas funcionalidades.

## ğŸ—ï¸ Arquitectura

El proyecto estÃ¡ organizado en capas siguiendo Clean Architecture:

```
ingestor_scrapper/
â”œâ”€ core/                    # Dominio (framework-agnÃ³stico)
â”‚  â”œâ”€ entities.py          # Modelos del dominio (Item, Page, Document, Record, ContentType)
â”‚  â””â”€ ports.py             # Interfaces (HtmlFetcher, DocumentFetcher, HtmlParser, 
â”‚                           #            TabularParser, PdfParser, Normalizer, OutputPort)
â”‚
â”œâ”€ application/            # Casos de uso (orquestan puertos)
â”‚  â”œâ”€ use_cases.py         # Casos de uso base y genÃ©ricos
â”‚  â”œâ”€ bcra_use_case.py      # BcraUseCase
â”‚  â”œâ”€ parser_router.py     # ParserRouter (selecciona parser por ContentType)
â”‚  â””â”€ universal_ingest_use_case.py  # UniversalIngestUseCase (mÃºltiples formatos)
â”‚
â”œâ”€ adapters/               # Implementaciones (dependientes de frameworks)
â”‚  â”œâ”€ fetcher_scrapy.py    # AdapterScrapyFetcher, AdapterScrapyDocumentFetcher
â”‚  â”œâ”€ fetcher_http.py      # AdapterHttpFetcher (stub para requests)
â”‚  â”œâ”€ parser_bs4.py       # AdapterBs4Parser (stub, requiere beautifulsoup4)
â”‚  â”œâ”€ parser_bcra.py       # AdapterBcraParser (funciona)
â”‚  â”œâ”€ parser_csv.py        # AdapterCsvParser (stub bÃ¡sico)
â”‚  â”œâ”€ parser_excel.py      # AdapterExcelParser (stub, requiere openpyxl)
â”‚  â”œâ”€ parser_pdf.py        # AdapterPdfParser (stub, requiere pdfplumber)
â”‚  â”œâ”€ normalizer_bcra.py   # AdapterBcraNormalizer
â”‚  â”œâ”€ normalizer_generic.py  # AdapterGenericNormalizer (fallback)
â”‚  â”œâ”€ output_stdout.py     # AdapterStdoutOutput
â”‚  â”œâ”€ output_json.py       # AdapterJsonOutput
â”‚  â””â”€ registry.py          # PARSER_REGISTRY (registro centralizado)
â”‚
â””â”€ interface/              # Entrada/Delivery (spiders, CLI)
   â””â”€ spiders/
      â”œâ”€ bcra_spider.py     # Spider para BCRA (funciona)
      â””â”€ universal_spider.py  # Spider genÃ©rico con ParserRouter (ejemplo)
```

### PatrÃ³n Puertos y Adaptadores

- **Puertos (Ports)**: Interfaces/Protocolos definidos en `core/ports.py` que representan contratos abstractos.
- **Adaptadores (Adapters)**: Implementaciones concretas en `adapters/` que implementan esos puertos usando frameworks especÃ­ficos (Scrapy, BeautifulSoup, etc.).

Esto permite que la lÃ³gica de negocio (`application/`) permanezca independiente de frameworks externos.

### Soporte para MÃºltiples Formatos

El proyecto ahora soporta mÃºltiples formatos de documentos:
- **HTML**: Parsing con BeautifulSoup4 (stub, requiere instalaciÃ³n)
- **CSV**: Parsing con mÃ³dulo `csv` nativo (stub bÃ¡sico)
- **Excel (XLS/XLSX)**: Parsing con openpyxl/xlrd (stub, requiere instalaciÃ³n)
- **PDF**: Parsing con pdfplumber/tabula-py (stub, requiere instalaciÃ³n)

El **ParserRouter** selecciona automÃ¡ticamente el parser correcto segÃºn el Content-Type del documento.

## ğŸ“š DocumentaciÃ³n

- ğŸ“– [Arquitectura Escalable](docs/ARQUITECTURA_SCALABLE.md) - GuÃ­a completa de la arquitectura y cÃ³mo agregar nuevos sitios/formatos
- ğŸ•·ï¸ [CÃ³mo Funciona Scrapy](docs/COMO_SCRAPY_FUNCIONA.md) - ExplicaciÃ³n de cÃ³mo Scrapy pasa el response al spider
- ğŸ” [CÃ³mo Scrapy Busca Variables](docs/COMO_SCRAPY_BUSCA_VARIABLES.md) - CÃ³mo Scrapy encuentra y usa las variables del spider
- ğŸ¯ [Para QuÃ© Sirve el Normalizer](docs/PARA_QUE_SIRVE_NORMALIZER.md) - ExplicaciÃ³n del rol del Normalizer en la arquitectura

## ğŸš€ InstalaciÃ³n

### 1. Crear entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate
```

### 2. Instalar dependencias

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Uso

### Ejecutar el spider de BCRA

```bash
scrapy crawl bcra
```

Este comando ejecutarÃ¡ el `bcra_spider` que extrae datos financieros de `https://www.bcra.gob.ar/PublicacionesEstadisticas/Principales_variables.asp` y genera un archivo JSON (`bcra_data.json`) con los resultados estructurados.

### Ejecutar el spider universal

```bash
scrapy crawl universal -a url="https://example.com"
```

Este spider puede manejar mÃºltiples formatos (HTML, CSV, Excel, PDF) automÃ¡ticamente usando el ParserRouter.

### Crear un nuevo spider

Para crear un nuevo spider, consulta la documentaciÃ³n: [Arquitectura Escalable](docs/ARQUITECTURA_SCALABLE.md)

Ejemplo bÃ¡sico:

```python
from ingestor_scrapper.adapters.fetchers import AdapterScrapyDocumentFetcher
from ingestor_scrapper.adapters.parsers import AdapterBcraParser
from ingestor_scrapper.adapters.normalizers import AdapterBcraNormalizer
from ingestor_scrapper.adapters.outputs import AdapterJsonOutput
from ingestor_scrapper.application.bcra_use_case import BcraUseCase

class MiSpider(scrapy.Spider):
    name = "mi_spider"
    start_urls = ["https://example.com"]
    
    def parse(self, response):
        fetcher = AdapterScrapyDocumentFetcher(response)
        parser = AdapterBcraParser()  # O tu parser especÃ­fico
        normalizer = AdapterBcraNormalizer()  # O tu normalizer especÃ­fico
        output = AdapterJsonOutput()
        
        use_case = BcraUseCase(fetcher, parser, normalizer, output)
        items = use_case.execute(response.url)
```

## ğŸ“¦ Estructura del Proyecto

- **`core/`**: Capa de dominio con entidades y puertos (interfaces). Framework-agnÃ³stico.
- **`application/`**: Casos de uso que orquestan los puertos para cumplir requisitos de negocio.
- **`adapters/`**: Implementaciones concretas de los puertos usando frameworks externos (Scrapy, BeautifulSoup, etc.).
- **`interface/`**: Puntos de entrada (spiders de Scrapy, futuros CLI, APIs, etc.).

## ğŸ—ºï¸ Roadmap

### PrÃ³ximos pasos sugeridos:

1. **Agregar nuevos parsers**: Crear parsers especÃ­ficos para nuevos sitios
   - Ver [Arquitectura Escalable](docs/ARQUITECTURA_SCALABLE.md) para guÃ­a completa

2. **Implementar parsers de stubs**: Completar implementaciÃ³n de parsers para CSV, Excel, PDF
   - Instalar dependencias necesarias (beautifulsoup4, openpyxl, pdfplumber)
   - Implementar lÃ³gica de parsing en los stubs

3. **Pipelines de Scrapy**: Activar pipelines para procesamiento de items
   - Descomentar secciÃ³n de pipelines en `settings.py`
   - Crear pipelines para validaciÃ³n, limpieza, almacenamiento

4. **Storage**: Agregar adaptadores de salida a archivos/base de datos
   - `AdapterDatabaseOutput` para persistir en DB
   - `AdapterApiOutput` para enviar a APIs

5. **Tests**: Agregar tests unitarios para cada capa
   - Tests de use cases
   - Tests de adaptadores (mocks)
   - Tests de integraciÃ³n

## ğŸ“ Notas

- El proyecto sigue el patrÃ³n de **parser por proveedor/sitio** para mÃ¡xima flexibilidad.
- Para scrapear un nuevo sitio, consulta [Arquitectura Escalable](docs/ARQUITECTURA_SCALABLE.md).
- Todos los archivos incluyen **TODOs** donde se puede expandir la funcionalidad.

## ğŸ“š Referencias

- [Scrapy Documentation](https://docs.scrapy.org/)
- [Clean Architecture (Hexagonal Architecture)](https://alistair.cockburn.us/hexagonal-architecture/)
- [Ports & Adapters Pattern](https://herbertograca.com/2017/11/16/explicit-architecture-01-ddd-hexagonal-onion-clean-cqrs-how-i-put-it-all-together/)
